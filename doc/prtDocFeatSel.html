
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Feature selection in the Pattern Recognition Toolbox</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-12-13"><meta name="DC.source" content="prtDocFeatSel.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Feature selection in the Pattern Recognition Toolbox</h1><!--introduction--><p>Feature selection is a technique that is used to determine which features of a data set are most relevant to performing classification. In general, for a fixed size training set, incorporating more features in classifier training can lead to declining performance due to the curse of dimensionality.  Therefore, it is often desirable to reduce the amount of features used to perform classification to the smallest amount possible that still gives the desired performance. prtFeatSel objects provide a way to select features based on a performance criteria.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Exhaustive feature selection</a></li><li><a href="#4">Sequential forward selection</a></li></ul></div><h2>Exhaustive feature selection<a name="1"></a></h2><pre>Exhaustive feature selection works by exhaustively comparing all
possible combinations of features that could be selected. The number of
features to be selected must be determined beforehand. The feature
selection algorithm will then evaluate the classifier with all
combinations of features and select the set that gives the best
performance. The classifier performance is determined by specifying a
prtEval object. Note that the computational complexity of exhaustive
feature selection grows very rapidly, so that for even moderately sized
data sets, exhaustive feature selection is impractical.  The following
example illustrates exhaustive feature selection:</pre><pre class="codeinput"><span class="comment">% Generate a data set, this data set has a redundant feature intentionally</span>
<span class="comment">% inserted for example purposes.</span>
dataSet = prtDataGenFeatureSelection;

featSel = prtFeatSelExhaustive;   <span class="comment">% Create a feature selction object</span>
featSel.nFeatures = 3;            <span class="comment">% Select three features of the data</span>

 <span class="comment">% The classification will be done using a prtClassGlrt object.</span>
classifier = prtClassGlrt;

<span class="comment">%   Change the evaluation metric to prtScoreAuc, the area under the</span>
<span class="comment">%   receiver operating curve.</span>
featSel.evaluationMetric = @(DS)prtEvalAuc(classifier, DS);

featSel = featSel.train(dataSet);  <span class="comment">% Train the feature selection object</span>
outDataSet = featSel.run(dataSet); <span class="comment">% Run the classifier on the data set</span>
                                   <span class="comment">% using only the selected features</span>
</pre><p>The above example illustrates several important things about feature selection. First, the clasification algorithm must be specified, as is done by passing a prtClassGlrt object to the prtEval function. You can instantiate and configure the prtClass object just like any other prtClass object. Second, the evaluation is done by a prtEval object, which takes the classifier and data set, and evaluates it in terms of a given performance metric.</p><p>By calling the train method on the prtFeatSel object, all possible combinations of 3 features are evaluated, and the set resulting in the best performance, is selected. The selected features are:</p><pre class="codeinput">featSel.selectedFeatures
</pre><pre class="codeoutput">ans =
     1     3     7
</pre><p>Finally, calling the run function on the dataset, runs the prtGlrt classifier object, but only using the features that were found during training, and outputs the result.</p><h2>Sequential forward selection<a name="4"></a></h2><p>Exhaustive feature selection can take a considerable amount of time to train, particularly if the number of features is large. Sequential feature selection can potentially resolve this problem while reducing the amount of training time. Sequential forward selection starts by selecting one feature by and evaluating the classifier on each feature individually. The feature that gives the best performance is then selected. If more than one feature is requested, the feature selection object will then repeat this, adding features, and selecting the one that most improves performance, until the desired number of features has been met. For example:</p><pre class="codeinput"><span class="comment">% Create a sequential forward selection object</span>
featSel = prtFeatSelSfs;

<span class="comment">% For fair comparision, leave the evaluation and classifier the same:</span>
featSel.evaluationMetric = @(DS)prtEvalAuc(classifier, DS);

featSel = featSel.train(dataSet);  <span class="comment">% Train the feature selection object</span>
featSel.selectedFeatures
</pre><pre class="codeoutput">ans =
     7     3     1
</pre><p>Observe that the selected features are [3 1 7]. Notice that they are the same features as found in the exhaustive feature selection, but in different order. This is because feature 3 contributes the most to the performance metric, follwed by feature 1, then feature 7. Note that it is not guaranteed that sequential forward selection will select the same features as exhaustive selection, it is merely a coincidence in this case. Sequential forward selection does not evaluate all possible combinations of features as exhaustive feature selection does.</p><pre class="codeinput"><span class="comment">% All feature selection objects in the Pattern Recognition Toolbox have the</span>
<span class="comment">% same API as discussed above.  For a list of all the different objects,</span>
<span class="comment">% and links to their individual help entries, &lt;prtDocFunctionList.html#8 A</span>
<span class="comment">% list of commonly used functions&gt;</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% Feature selection in the Pattern Recognition Toolbox
% 
% Feature selection is a technique that is used to determine which features
% of a data set are most relevant to performing classification. In general,
% for a fixed size training set, incorporating more features in classifier
% training can lead to declining performance due to the curse of
% dimensionality.  Therefore, it is often desirable to reduce
% the amount of features used to perform classification to the smallest
% amount possible that still gives the desired performance. prtFeatSel
% objects provide a way to select features based on a performance criteria.

%% Exhaustive feature selection
%
%  Exhaustive feature selection works by exhaustively comparing all
%  possible combinations of features that could be selected. The number of
%  features to be selected must be determined beforehand. The feature
%  selection algorithm will then evaluate the classifier with all
%  combinations of features and select the set that gives the best
%  performance. The classifier performance is determined by specifying a
%  prtEval object. Note that the computational complexity of exhaustive
%  feature selection grows very rapidly, so that for even moderately sized
%  data sets, exhaustive feature selection is impractical.  The following
%  example illustrates exhaustive feature selection:

% Generate a data set, this data set has a redundant feature intentionally
% inserted for example purposes.
dataSet = prtDataGenFeatureSelection;

featSel = prtFeatSelExhaustive;   % Create a feature selction object
featSel.nFeatures = 3;            % Select three features of the data

 % The classification will be done using a prtClassGlrt object.
classifier = prtClassGlrt;       

%   Change the evaluation metric to prtScoreAuc, the area under the
%   receiver operating curve.
featSel.evaluationMetric = @(DS)prtEvalAuc(classifier, DS);

featSel = featSel.train(dataSet);  % Train the feature selection object
outDataSet = featSel.run(dataSet); % Run the classifier on the data set 
                                   % using only the selected features
                                   
%%
% The above example illustrates several important things about feature
% selection. First, the clasification algorithm must be specified, as is
% done by passing a prtClassGlrt object to the prtEval function. You can
% instantiate and configure the prtClass object just like any other
% prtClass object. Second, the evaluation is done by a prtEval object,
% which takes the classifier and data set, and evaluates it in terms of a
% given performance metric.
%
% By calling the train method on the prtFeatSel object, all possible
% combinations of 3 features are evaluated, and the set resulting in the
% best performance, is selected. The selected features are:

featSel.selectedFeatures

%%
% Finally, calling the run function on the dataset, runs the prtGlrt
% classifier object, but only using the features that were found during
% training, and outputs the result.

%% Sequential forward selection
% Exhaustive feature selection can take a considerable amount of time to
% train, particularly if the number of features is large. Sequential
% feature selection can potentially resolve this problem while reducing the
% amount of training time. Sequential forward selection starts by selecting
% one feature by and evaluating the classifier on each feature
% individually. The feature that gives the best performance is then
% selected. If more than one feature is requested, the feature selection
% object will then repeat this, adding features, and selecting the one that
% most improves performance, until the desired number of features has been
% met. For example:

% Create a sequential forward selection object
featSel = prtFeatSelSfs;

% For fair comparision, leave the evaluation and classifier the same:
featSel.evaluationMetric = @(DS)prtEvalAuc(classifier, DS);

featSel = featSel.train(dataSet);  % Train the feature selection object
featSel.selectedFeatures

%%
% Observe that the selected features are [3 1 7]. Notice that they are the
% same features as found in the exhaustive feature selection, but in
% different order. This is because feature 3 contributes the most to the
% performance metric, follwed by feature 1, then feature 7. Note that it is
% not guaranteed that sequential forward selection will select the same
% features as exhaustive selection, it is merely a coincidence in this
% case. Sequential forward selection does not evaluate all possible
% combinations of features as exhaustive feature selection does.

% All feature selection objects in the Pattern Recognition Toolbox have the
% same API as discussed above.  For a list of all the different objects,
% and links to their individual help entries, <prtDocFunctionList.html#8 A
% list of commonly used functions>
##### SOURCE END #####
--></body></html>